{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T06:56:55.982457Z",
     "start_time": "2025-03-26T06:56:53.354865Z"
    }
   },
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain_community langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters langchain_openai"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:05:26.427181Z",
     "start_time": "2025-03-26T07:05:24.733822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(verbose=True)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "# print(f\"\"\"\n",
    "# - OPENAI_API_KEY:{OPENAI_API_KEY}\n",
    "# - LANGCHAIN_API_KEY:{LANGCHAIN_API_KEY}\n",
    "# - TAVILY_API_KEY: {TAVILY_API_KEY}\n",
    "# \"\"\")"
   ],
   "id": "11ed6c7eb7f79147",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/sangpire/miniconda3/lib/python3.12/site-packages (1.0.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:05:35.891843Z",
     "start_time": "2025-03-26T07:05:28.365714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tavily import TavilyClient\n",
    "tavily = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "\n",
    "response = tavily.search(query=\"Where does Messi play right now?\", max_results=3)\n",
    "context = [{\"url\": obj[\"url\"], \"content\": obj[\"content\"]} for obj in response['results']]\n",
    "\n",
    "# You can easily get search result context based on any max tokens straight into your RAG.\n",
    "# The response is a string of the context within the max_token limit.\n",
    "\n",
    "response_context = tavily.get_search_context(query=\"Where does Messi play right now?\", search_depth=\"advanced\", max_tokens=500)\n",
    "\n",
    "# You can also get a simple answer to a question including relevant sources all with a simple function call:\n",
    "# You can use it for baseline\n",
    "response_qna = tavily.qna_search(query=\"Where does Messi play right now?\")"
   ],
   "id": "3fa3d1b80b751ba4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:05:38.215898Z",
     "start_time": "2025-03-26T07:05:38.175975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature = 0)"
   ],
   "id": "e28dbcc1096b414e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:05:45.904351Z",
     "start_time": "2025-03-26T07:05:43.010447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Index\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "f350399100bee682",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:11:31.520323Z",
     "start_time": "2025-03-26T07:11:30.376092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Retrieval Grader\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system = \"\"\"You are a grader assessing relevance\n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "    \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"question: {question}\\n\\n document: {document} \"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"What is prompt?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[0].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ],
   "id": "27a14c9e8c2801bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:23:09.719627Z",
     "start_time": "2025-03-26T07:23:05.375525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Generate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system = \"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"question: {question}\\n\\n context: {context} \"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What is prompt?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ],
   "id": "4ff1e716c19db7e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A prompt is a method used in prompt engineering, also known as In-Context Prompting, to communicate with language models (LLMs) to guide their behavior towards desired outcomes without altering the model's weights. It involves experimentation and heuristics, as the effectiveness of prompts can vary significantly among different models. The primary goal is to achieve alignment and steerability of the model.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:26:22.236720Z",
     "start_time": "2025-03-26T07:26:21.545544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Answer Grader\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an\n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is\n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"question: {question}\\n\\n answer: {generation} \"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ],
   "id": "953d44cd45ddb9ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:23:13.266098Z",
     "start_time": "2025-03-26T07:23:12.041403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "system = \"\"\"You are a grader assessing whether\n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
    "    single key 'score' and no preamble or explanation.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"documents: {documents}\\n\\n answer: {generation} \"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ],
   "id": "36352eead650e41c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:05:50.328843Z",
     "start_time": "2025-03-26T07:05:50.326627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        answer: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    answer: str\n",
    "    web_search: str\n",
    "    hallucination: str\n",
    "    documents: List[str]"
   ],
   "id": "4c3e823ac2a44211",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:06:51.916835Z",
     "start_time": "2025-03-26T07:06:51.902226Z"
    }
   },
   "cell_type": "code",
   "source": "retriever",
   "id": "162b1853bc5c61bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x167144da0>, search_kwargs={})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:19:20.546515Z",
     "start_time": "2025-03-26T07:19:20.540125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.graph import END\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def doc_retrieval(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    return {\n",
    "        \"documents\": retriever.get_relevant_documents(question)\n",
    "    }\n",
    "\n",
    "def relevance_checker(state: GraphState):\n",
    "    # print(f\"@relevance_checker:{state}\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"documents\": filtered_docs,\n",
    "        \"web_search\": web_search\n",
    "    }\n",
    "\n",
    "def generate_answer(state: GraphState):\n",
    "\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "    return {\n",
    "        \"answer\": \"Paris\"\n",
    "    }\n",
    "\n",
    "def hallucination_checker(state: GraphState):\n",
    "    # print(f\"@hallucination_checker:{state}\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    return {\n",
    "        \"hallucination\": \"no\",\n",
    "        \"answer\": state[\"answer\"]\n",
    "    }\n",
    "\n",
    "def search_tavily(state: GraphState):\n",
    "    \"\"\"\n",
    "    Searches the web for relevant documents.\n",
    "\n",
    "    Args:\n",
    "        state: the state of the graph\n",
    "\n",
    "    Returns:\n",
    "        the updated state\n",
    "    \"\"\"\n",
    "    # print(f\"@search_tavily:{state}\")\n",
    "    question = state[\"question\"]\n",
    "    documents = None\n",
    "\n",
    "    docs = tavily.search(query=question)['results']\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"documents\": documents\n",
    "    }\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generates a response to the question.\n",
    "\n",
    "    Args:\n",
    "        state: the state of the graph\n",
    "\n",
    "    Returns:\n",
    "        the updated state\n",
    "    \"\"\"\n",
    "    # print(f\"@generate:{state}\")\n",
    "\n",
    "    return {\n",
    "        \"generation\": \"response\"\n",
    "    }\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "\n",
    "    print(f\"@grade_generation_v_documents_and_question:{state}\")\n",
    "\n",
    "    return \"useful\""
   ],
   "id": "eae4863be180914c",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T07:21:34.250508Z",
     "start_time": "2025-03-26T07:21:27.255908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.set_entry_point(\"docRetrieval\")\n",
    "\n",
    "workflow.add_node(\"docRetrieval\", doc_retrieval)\n",
    "workflow.add_node(\"relevanceChecker\", relevance_checker)\n",
    "workflow.add_node(\"searchTavily\", search_tavily)\n",
    "workflow.add_node(\"generateAnswer\", generate_answer)\n",
    "workflow.add_node(\"hallucinationChecker\", hallucination_checker)\n",
    "\n",
    "workflow.add_edge(\"docRetrieval\", \"relevanceChecker\")\n",
    "workflow.add_edge(\"searchTavily\", \"relevanceChecker\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevanceChecker\",\n",
    "    lambda state: state[\"web_search\"],\n",
    "    {\n",
    "        \"Yes\": \"searchTavily\",\n",
    "        \"No\": \"generateAnswer\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"generateAnswer\", \"hallucinationChecker\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"hallucinationChecker\",\n",
    "    lambda state: state[\"hallucination\"],\n",
    "    {\n",
    "        \"yes\": \"generateAnswer\",\n",
    "        \"no\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()\n",
    "inputs = {\n",
    "    \"question\": \"What is the capital of France?\"\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"STEP {key}\")\n",
    "\n",
    "print(value[\"answer\"])"
   ],
   "id": "6f54edb0728049a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP docRetrieval\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "STEP relevanceChecker\n",
      "STEP searchTavily\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "STEP relevanceChecker\n",
      "STEP generateAnswer\n",
      "STEP hallucinationChecker\n",
      "Paris\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
